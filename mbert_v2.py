# -*- coding: utf-8 -*-
"""mBERT_v2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iXZrzWRpmVRJ4UODErbsfoDUsawrcY5c
"""

import pandas as pd
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
from torch.optim import AdamW
#from torch.optim import Adagrad
from torch.utils.data import DataLoader, Dataset
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Nuskaitome duomenis
df_user_stories_train = pd.read_csv('https://raw.githubusercontent.com/EvelinaGed/LT_userstories/refs/heads/main/Training_user_stories_LT_v4.csv')
df_user_stories_test = pd.read_csv('https://raw.githubusercontent.com/EvelinaGed/LT_userstories/refs/heads/main/test_set_LT_v4.csv')

# BERT modelio ir tokenizer įkėlimas
model_name = "bert-base-multilingual-cased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)

# Įranga
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Sukuriame PyTorch Dataset
class UserStoriesDataset(Dataset):
    def __init__(self, texts, labels, tokenizer):
        self.encodings = tokenizer(texts, truncation=True, padding=True, return_tensors="pt")
        self.labels = torch.tensor(labels, dtype=torch.float)  # pakeista į float

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        item = {key: val[idx] for key, val in self.encodings.items()}
        item['labels'] = self.labels[idx]
        return item

# Paruošiame duomenis su visais trimis kriterijais
train_labels = df_user_stories_train.iloc[:, 1:4].values.tolist()
test_labels = df_user_stories_test.iloc[:, 1:4].values.tolist()

train_dataset = UserStoriesDataset(
    texts=df_user_stories_train['User stories'].tolist(),
    labels=train_labels,
    tokenizer=tokenizer
)

test_dataset = UserStoriesDataset(
    texts=df_user_stories_test['User stories'].tolist(),
    labels=test_labels,
    tokenizer=tokenizer
)

train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True) #batch galima keisti, optimizer, epochų skaičių, adamw, neuronų skaičių.
test_loader = DataLoader(test_dataset, batch_size=8)

# Nuostolių funkcija ir optimizatorius
optimizer = AdamW(model.parameters(), lr=5e-5)
#optimizer = Adagrad(model.parameters(), lr=1e-2)

loss_fn = torch.nn.BCEWithLogitsLoss()

# Modelio treniravimas
model.train()
epochs = 3

for epoch in range(epochs):
    total_loss = 0
    for batch in train_loader:
        inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}
        labels = batch['labels'].to(device)

        optimizer.zero_grad()
        outputs = model(**inputs)
        loss = loss_fn(outputs.logits, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch {epoch + 1}, Loss: {total_loss:.4f}")

# Modelio testavimas
model.eval()
all_preds = []
all_labels = []

with torch.no_grad():
    for batch in test_loader:
        inputs = {key: val.to(device) for key, val in batch.items() if key != 'labels'}
        labels = batch['labels'].to(device)

        outputs = model(**inputs)
        preds = torch.sigmoid(outputs.logits).cpu().numpy()  # sigmoid transformacija
        preds = (preds > 0.5).astype(int)  # apvalinimas iki 0 arba 1

        all_preds.extend(preds)
        all_labels.extend(labels.cpu().numpy())

# Konvertuojame į numpy masyvus
all_labels = np.array(all_labels)
all_preds = np.array(all_preds)

# Bendras tikslumas
accuracy = accuracy_score(all_labels, all_preds)
print(f"Test accuracy: {accuracy:.2f}")

# Kiekvieno kriterijaus metrikos
label_names = ["Atomiškumas", "Pilnumas", "Dviprasmiškumas"]

for i, label in enumerate(label_names):
    acc = accuracy_score(all_labels[:, i], all_preds[:, i])
    prec = precision_score(all_labels[:, i], all_preds[:, i], zero_division=0)
    rec = recall_score(all_labels[:, i], all_preds[:, i], zero_division=0)
    f1 = f1_score(all_labels[:, i], all_preds[:, i], zero_division=0)

    print(f"\n{label}:")
    print(f"  Tikslumas (Accuracy):  {acc:.2f}")
    print(f"  Preciziškumas:         {prec:.2f}")
    print(f"  Atkūrimas (Recall):    {rec:.2f}")
    print(f"  F1 rezultatas:         {f1:.2f}")

# Confusion matricos vizualizavimas
for i, label in enumerate(label_names):
    cm = confusion_matrix(all_labels[:, i], all_preds[:, i])
    plt.figure(figsize=(4, 3))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False)
    plt.title(f"Confusion matrica – {label}")
    plt.xlabel("Prognozuota reikšmė")
    plt.ylabel("Tikroji reikšmė")
    plt.tight_layout()
    plt.show()